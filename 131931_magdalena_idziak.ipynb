{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maagdaidziak/ADN/blob/main/131931_magdalena_idziak.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Wstęp"
      ],
      "metadata": {
        "id": "MqktrpNysSYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "W projekcie podjęłam się predykcji cen mieszkań na wynajem w Poznaniu (nick na Kaggle Magdalena Idziak, wynik 112342.75),\n",
        "\n",
        "Brakujące dane zaimputowałam na postawie tekstu z innych kolumna (np. liczbę pokoi, dzielnicę z tytułu ogłoszenia), metodą knn, oraz średniej. Dla danych typu bool brakujące kolumny uzupełniłam jako FALSE.\n",
        "\n",
        "Następnie do predykcji najlepszą metodą okazała się optymalizacja Bayesa (BayesSearchCV), która jest alternatywą dla Grid Search przy szukaniu optymalnych parametrów w XGBoost.\n",
        "\n",
        "Przy pisaniu kodu korzystałam z czatu GPT, jednak pomagał on w pisaniu małych fragmentów kodu. W moim doświadczeniu czat najlepiej radzi sobie z małymi zadaniami, które następnie trzeba włączyć do własnego programu.\n"
      ],
      "metadata": {
        "id": "e59FNoL8Zz2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Metodyka"
      ],
      "metadata": {
        "id": "EBlQQGYfsSS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Program składa się z 3 głównych części: main, preprocess_data i train_model_bayes_search (oraz nieużyty finalnie train_model, który używał GridSearcha. Poniżej opisałam co zawiera każda część.\n",
        "\n",
        "\n",
        "2. Przetwarzanie danych:\n",
        "\n",
        "- Dodanie kolumny roku na podstawie 'date_active'.\n",
        "\n",
        "- Poprawienie typów danych na prawidłowe (object na bool), zamiana brakujących i nieprawidłowych wartości (np liczba pokoi <1, liczba metrów <0) na np.nan.\n",
        "\n",
        "- W kolumnach o typie danych \"bool\" zaimputowanie \"FALSE\" dla wszystkich brakujących wartości.\n",
        "\n",
        "\n",
        "- Dla flat_rooms\n",
        "  - imputacja wartości \"1\"  dla rekordów z \"kawale*\" w opisie.\n",
        "\n",
        "  - Imputacja  flat_rooms na podstawie ad_title (rekordy z fragmentem \"pokoj*\") przy pomocy biblioteki \"re\" funkcji extract_room_count.\n",
        "\n",
        "  - Imputacja flat_rooms dla rekordów z flat_area > 0 na podstawie knn = 3 z flat_area.\n",
        "\n",
        "  - Imputacja średnią dla pozostałych braków w flat_rooms.\n",
        "\n",
        "- Dla flat_area:\n",
        "  - Imputacja metodą knn = 3 na podstawie flat_rooms. Założenie, że mieszkania o tej samej liczbie pokoi mogą mieć podobną wielkość.\n",
        "\n",
        "- Dla quarter:\n",
        "   - Imputacja brakującej kolumny quarter na podstawie ad_title. Jeżeli ad_title zawiera słowo należace do zbioru dzielnic z kolumny quarter, imputujemy je.\n",
        "\n",
        "   - Dla pozostałych dzielnic, imputacja dzielnicy \"inne\"\n",
        "\n",
        "\n",
        "3. Trenowanie modelu:\n",
        "\n",
        "- Najpierw użyłam funkcji liniowej.\n",
        "- Następnie Decision Tree i Random Forest przy pomocy Grid Search z pliku train_model.\n",
        "- Następnie użyłam Random Search, który jest szybszy w szukaniu najlepszych parametrów od Grid Search\n",
        "- Ostatecznie wybrałam BayesSearchCV (funkcja znajduje się w pliku train_model_bayes_search), który okazał się najlepszy w szukaniu optymalnych parametrów, jednak wyszukanie ich zabiera dużo czasu.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u0ntTzfwv2rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Wyniki"
      ],
      "metadata": {
        "id": "vlUj41SlsSNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trenowanie modelu i 10 predykcji. Wyniki zależały od sposobu imputacji danych i wybranego modelu. W nawiasach podałam wyniki MSE dla pliku train.\n",
        "\n",
        "1. Funkcja Liniowa (182225.62) dla\n",
        "        'flat_area', 'flat_rooms', 'year_activ'\n",
        "\n",
        "Imputacja flat_rooms z knn flat_area w przypadku rekordów z flat_area>0. Dla reszty średnia.\n",
        "Flat_area imputowane przy pomocy knn z flat_rooms.\n",
        "\n",
        "2. Grid Search dla\n",
        "        'flat_area', 'flat_rooms', 'year_activ'\n",
        "- Decision Tree (164590.03)\n",
        "- Random Forest (156813.83) dla parametrów: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50}\n",
        "\n",
        "3. Imputacja flat_rooms z ad_title (kawale* i regex dla większej liczby pokoi).\n",
        "Dla reszty rekordów flat_rooms ze średniej.\n",
        "Imputacja flat_area z knn flat_rooms.\n",
        "\n",
        "4. Dodanie kolumny 'year_active' z 'date_active'. Uwzględnienie jej w budowaniu modelu.\n",
        "\n",
        "4. Random Search - najlepszy wynik (109019.76) dla Random Forest dla parametrów.\n",
        "Dodanie\n",
        "        'flat_rent', 'flat_deposit', 'flat_for_students', 'building_floor_num', 'flat_balcony', 'flat_utility_room', 'flat_garage',\n",
        "        'flat_basement', 'flat_garden', 'flat_tarrace', 'flat_lift', 'flat_two_level', 'flat_kitchen_sep',\n",
        "        'flat_air_cond', 'flat_nonsmokers', 'flat_washmachine', 'flat_dishwasher', 'flat_fridge',\n",
        "        'flat_cooker', 'flat_oven', 'flat_internet', 'flat_television', 'flat_anti_blinds',\n",
        "        'flat_monitoring', 'flat_closed_area'\n",
        "\n",
        "{'max_depth': 22, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
        "\n",
        "5. Imputacja brakujących 'quarter' na podstawie ad_title. Dla reszty rekordów imputacja 'other'. Użycie danych typu dummies.\n",
        "\n",
        "6. BayesSearchCV (XGBoost) w train_model_bayes_search:\n",
        "- Decision Tree: (130202.42)\n",
        "- Random Forest: (86920.66) dla parametrów: {'max_depth': 45, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}"
      ],
      "metadata": {
        "id": "KgyRdKZGY-IF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Podsumowanie"
      ],
      "metadata": {
        "id": "cLjVkczMsSFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bardzo częstym problemem w analizie danych są braki danych. W tym projekcie wykorzystałam wiedzę z zakresu analizy danych niekompletnych w celu imputacji brakujących danych różnymi metodami.\n",
        "\n",
        "Przed analizą i imputacją danych konieczne jest bardzo dobre zrozumienie danych i ich kontekstu. Bez tego nie jesteśmy również w stanie efektywnie korzystać z LLMów, które potrzebują jasnych instrukcji i fragmentowania kodu.\n",
        "Dobre zrozumienie danych, wybranie metod do imputacji, metody budowy modelu i umiejętność korzystania z LLM-ów, umożliwiły mi poprawę modelu i otrzymanie wyniku 112342.75 na Kaggle."
      ],
      "metadata": {
        "id": "RjMW4YQ5ckZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Kody do odtworzenia wyników"
      ],
      "metadata": {
        "id": "y5vxmRoNsc6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip install scikit-optimize # instalacja wymaganych modułów\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import os\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "#  Funkcja przetwarzania danych\n",
        "def preprocess_data(data, columns_object_dtype, columns_with_conditions):\n",
        "    \"\"\"Przetwarza dane, wypełnia braki i przygotowuje je do modelowania.\"\"\"\n",
        "    data['year_activ'] = pd.to_datetime(data['date_activ'], errors = 'coerce').dt.year\n",
        "    data['year_activ'] = data['year_activ'].fillna(data['year_activ'].mode()[0])\n",
        "\n",
        "\n",
        "    # Zamiana wartości w kolumnie na typ bool\n",
        "    for column in data.select_dtypes(include='object').columns:\n",
        "        if column not in columns_object_dtype:\n",
        "            pd.set_option('future.no_silent_downcasting', True)\n",
        "            data[column] = data[column].fillna(False).astype(bool)\n",
        "\n",
        "    # Zamiana braków danych lub wartości mniejszych od 0\n",
        "    data = data.replace([None, '', 'NA', 'N/A', 'null'], np.nan)\n",
        "    for column, condition in columns_with_conditions.items():\n",
        "        data.loc[condition(data[column]), column] = np.nan\n",
        "\n",
        "    # Sprawdzenie ad_title i imputacja wartości \"1\" w kolumnie \"flat_rooms\" dla kawalerek\n",
        "    data.loc[data['flat_rooms'].isna() & data['ad_title'].str.contains('(?i)kawale', na=False), 'flat_rooms'] = 1\n",
        "\n",
        "    # Sprawdzanie ad_title i imputacja liczby pokoi na podstawie liczby przed \"pok\"\n",
        "    def extract_room_count(title):\n",
        "        match = re.search(r'(\\d)\\s*-?\\s*pok(?:ojowe|oje|\\.|\\s)', title, re.IGNORECASE)\n",
        "        return int(match.group(1)) if match else np.nan\n",
        "\n",
        "    data.loc[data['flat_rooms'].isna(), 'flat_rooms'] = data['ad_title'].apply(\n",
        "        lambda x: extract_room_count(x) if isinstance(x, str) else np.nan)\n",
        "\n",
        "    # Imputacja flat_rooms na podstawie knn z flat_area\n",
        "    mask_missing_rooms = data[\"flat_rooms\"].isna() & (data[\"flat_area\"] > 0)\n",
        "    data_knn_rooms = data[['flat_area', 'flat_rooms']]\n",
        "    imputer = KNNImputer(n_neighbors=3)\n",
        "    data_knn_imputed_rooms = imputer.fit_transform(data_knn_rooms)\n",
        "    data.loc[mask_missing_rooms, \"flat_rooms\"] = data_knn_imputed_rooms[mask_missing_rooms, 1]\n",
        "\n",
        "    # Imputacja średnią dla pozostałych braków w flat_rooms\n",
        "    mean_imputer = SimpleImputer(strategy='mean')\n",
        "    data['flat_rooms'] = mean_imputer.fit_transform(data[['flat_rooms']])\n",
        "\n",
        "    # Imputacja flat_area na podstawie flat_rooms za pomocą KNNImputer\n",
        "    mask_missing_area = data[\"flat_area\"].isna() & (data[\"flat_rooms\"] > 0)\n",
        "    data_knn_area = data[['flat_rooms', 'flat_area']]\n",
        "    imputer_area = KNNImputer(n_neighbors=3)\n",
        "    data_knn_imputed_area = imputer_area.fit_transform(data_knn_area)\n",
        "    data.loc[mask_missing_area, \"flat_area\"] = data_knn_imputed_area[mask_missing_area, 1]\n",
        "\n",
        "\n",
        "    #Imutacja brakujacych wartości w kolumnie 'quarter'\n",
        "    unique_quarters = data['quarter'].dropna().unique()\n",
        "    def extract_quarter_from_title(ad_title):\n",
        "        if isinstance(ad_title, str):\n",
        "            for quarter in unique_quarters:\n",
        "                if str(quarter).lower() in ad_title.lower():\n",
        "                    return quarter\n",
        "        return np.nan\n",
        "    data['quarter'] = data['quarter'].fillna(data['ad_title'].apply(extract_quarter_from_title))\n",
        "    # Imputacja brakujących wartości w kolumnie 'quarter'\n",
        "    data['quarter'] = data['quarter'].fillna('inne')\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "#FUnkcja trenowania modelu\n",
        "def train_and_evaluate_models_with_bayessearch(models_with_params, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Trenuje modele z BayesSearchCV i ocenia je za pomocą MSE.\"\"\"\n",
        "    results = {}\n",
        "    predictions_dict = {}\n",
        "    for name, (model, search_space) in models_with_params.items():\n",
        "            bayes_search = BayesSearchCV(\n",
        "                estimator=model,\n",
        "                search_spaces=search_space,\n",
        "                scoring='neg_mean_squared_error',\n",
        "                n_iter=20,  # Liczba iteracji optymalizacji\n",
        "                cv=3,\n",
        "                verbose=1,\n",
        "                random_state=42\n",
        "            )\n",
        "            bayes_search.fit(X_train, y_train)\n",
        "            best_model = bayes_search.best_estimator_\n",
        "            predictions = best_model.predict(X_test)\n",
        "            mse = mean_squared_error(y_test, predictions)\n",
        "            results[name] = mse\n",
        "            predictions_dict[name] = predictions\n",
        "            print(f\"{name} - Najlepsze parametry: {bayes_search.best_params_}\")\n",
        "            print(f\"Najlepsze MSE: {-bayes_search.best_score_}\")\n",
        "    return results, predictions_dict\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "\n",
        "    #wczytanie danych do użycia funkcji preprocess_data\n",
        "    train_file_path= 'pzn-rent-train.csv'\n",
        "    test_final_file_path = 'pzn-rent-test.csv'\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "\n",
        "    # Przygotowanie danych do funkcji preprocess_data\n",
        "    columns_object_dtype = ['ad_title', 'date_activ', 'date_modif', 'date_expire', 'quarter']\n",
        "    columns_with_conditions = {\n",
        "        'flat_area': lambda x: x <= 0,\n",
        "        'flat_rooms': lambda x: x <= 0,\n",
        "        'flat_rent': lambda x: x < 0,\n",
        "        'flat_deposit': lambda x: x < 0\n",
        "    }\n",
        "\n",
        "    #przetworzenie danych (po else) lub pobranie wcześniej przetowrzonego pliku z danymi\n",
        "    modified_train_file = 'pzn-rent-train-modified.csv'\n",
        "    if os.path.exists(modified_train_file):\n",
        "        train_data = pd.read_csv(\"pzn-rent-train-modified.csv\")\n",
        "        print(\"wczytano przetwrzone dane z pliku\")\n",
        "    else:\n",
        "        train_data = preprocess_data(train_data, columns_object_dtype, columns_with_conditions)\n",
        "        train_data.to_csv(modified_train_file, index=False)\n",
        "        print(\"Przetworzono dane i zapisano do pliku.\")\n",
        "\n",
        "    #zamiana danych w kolumnie \"quarter\" na typ dummies\n",
        "    train_data = pd.get_dummies(train_data, columns=['quarter'], drop_first=True)\n",
        "\n",
        "    #przygotowanie cech\n",
        "    additional_features = [\n",
        "        'flat_rent', 'flat_deposit', 'flat_for_students', 'building_floor_num', 'flat_balcony', 'flat_utility_room', 'flat_garage',\n",
        "        'flat_basement', 'flat_garden', 'flat_tarrace', 'flat_lift', 'flat_two_level', 'flat_kitchen_sep',\n",
        "        'flat_air_cond', 'flat_nonsmokers', 'flat_washmachine', 'flat_dishwasher', 'flat_fridge',\n",
        "        'flat_cooker', 'flat_oven', 'flat_internet', 'flat_television', 'flat_anti_blinds',\n",
        "        'flat_monitoring', 'flat_closed_area'\n",
        "    ]\n",
        "    features = ['flat_area', 'flat_rooms', 'year_activ'] + [col for col in train_data.columns if col.startswith('quarter_')] + additional_features\n",
        "    print(\"Wykorzystywane cechy:\", features + additional_features)\n",
        "    target = 'price'\n",
        "    train_data = train_data.dropna(subset=features + [target])\n",
        "    X = train_data[features]\n",
        "    y = train_data[target]\n",
        "\n",
        "    # podział na zbiór treningowy i testowy pliky train_data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Definicja modeli\n",
        "    models_with_params = {\n",
        "         'Decision Tree': (DecisionTreeRegressor(random_state=42), {\n",
        "             'max_depth': (10, 50),\n",
        "             'min_samples_split': (2, 10),\n",
        "             'min_samples_leaf': (1, 5)\n",
        "        }),\n",
        "        'Random Forest': (RandomForestRegressor(random_state=42), {\n",
        "            'n_estimators': (50, 300),\n",
        "            'max_depth': (10, 50),\n",
        "            'min_samples_split': (2, 10),\n",
        "            'min_samples_leaf': (1, 5)\n",
        "        })\n",
        "    }\n",
        "\n",
        "\n",
        "    # Trenowanie modeli\n",
        "    results, predictions = train_and_evaluate_models_with_bayessearch(models_with_params, X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # Porównanie wyników\n",
        "    print(\"Porównanie MSE dla modeli:\")\n",
        "    for model_name, mse in results.items():\n",
        "        print(f\"{model_name}: {mse:.2f}\")\n",
        "\n",
        "    # Wybór najlepszego modelu\n",
        "    best_model_name = min(results, key=results.get)\n",
        "    print(f\"Najlepszy model: {best_model_name} z MSE: {results[best_model_name]:.2f}\")\n",
        "    # Analiza ważności cech dla najlepszego modelu (np. Random Forest)\n",
        "    best_model = models_with_params[best_model_name][0]\n",
        "\n",
        "\n",
        "    # Wczytanie danych testowych i predykcja\n",
        "    test_final_data = pd.read_csv(test_final_file_path)\n",
        "    test_final_data = preprocess_data(test_final_data, columns_object_dtype, columns_with_conditions)\n",
        "    test_final_data = pd.get_dummies(test_final_data, columns=['quarter'], drop_first=True)\n",
        "    test_final_data = test_final_data.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "    final_model = models_with_params[best_model_name][0]\n",
        "    final_model.fit(X_test, y_test)\n",
        "    predicted_prices = final_model.predict(test_final_data)\n",
        "\n",
        "    # Zapis wyników\n",
        "    output_test_final_data = pd.DataFrame({\n",
        "        'ID': range(1, len(predicted_prices) + 1),\n",
        "        'TARGET': predicted_prices\n",
        "    })\n",
        "    output_test_final_data.to_csv('pzn-rent-test-predicted.csv', index=False)\n",
        "    print(\"Predykcja zakończona. Wyniki zapisano w 'pzn-rent-test-predicted.csv'\")\n"
      ],
      "metadata": {
        "id": "ZGIwjfNMeKjd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}